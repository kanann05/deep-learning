{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80586b16-89c7-4958-afaf-af951a045cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.utils import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924128df-246f-4bec-aae4-639fdf1d10a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8e8b6d2-0500-4aeb-a1ee-5c972f0f945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\3457086812.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"Location\").apply(lambda g : g.ffill().bfill()).reset_index(drop = True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"rain.csv\", parse_dates = [\"Date\"])\n",
    "df = df.sort_values([\"Location\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "df[\"RainToday\"] = df[\"RainToday\"].map({\"Yes\" : 1, \"No\" : 0})\n",
    "df[\"RainTomorrow\"] = df[\"RainTomorrow\"].map({\"Yes\" : 1, \"No\" : 0})\n",
    "\n",
    "df = df.groupby(\"Location\").apply(lambda g : g.ffill().bfill()).reset_index(drop = True)\n",
    "\n",
    "numeric_cols = [\n",
    "    \"MinTemp\",\"MaxTemp\",\"Humidity9am\",\"Humidity3pm\",\n",
    "    \"Pressure9am\",\"Pressure3pm\",\"WindSpeed9am\",\"WindSpeed3pm\",\"RainToday\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ddbb6b9-82ef-40c5-b28f-06758f163d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(data, lookback, numeric_cols, target_col) :\n",
    "    x, y = [], []\n",
    "\n",
    "    for i in range (lookback, len(data)):\n",
    "        seq = df[numeric_cols].iloc[i - lookback : i].values\n",
    "        x.append(seq)\n",
    "        y.append(df[target_col].iloc[i])\n",
    "\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea66e97-5288-48b6-a0c8-66da7ceb441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Training for Bangalore--\n",
      "\n",
      "accuracy : 0.6585365853658537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.90      0.79        29\n",
      "           1       0.25      0.08      0.12        12\n",
      "\n",
      "    accuracy                           0.66        41\n",
      "   macro avg       0.48      0.49      0.46        41\n",
      "weighted avg       0.57      0.66      0.59        41\n",
      "\n",
      "--Training for Chennai--\n",
      "\n",
      "accuracy : 0.7105263157894737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83        27\n",
      "           1       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.36      0.50      0.42        38\n",
      "weighted avg       0.50      0.71      0.59        38\n",
      "\n",
      "--Training for Kolkata--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy : 0.6111111111111112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.85      0.76        26\n",
      "           1       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.61        36\n",
      "   macro avg       0.34      0.42      0.38        36\n",
      "weighted avg       0.50      0.61      0.55        36\n",
      "\n",
      "--Training for Mumbai--\n",
      "\n",
      "accuracy : 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.96      0.80        25\n",
      "           1       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.67        36\n",
      "   macro avg       0.34      0.48      0.40        36\n",
      "weighted avg       0.48      0.67      0.56        36\n",
      "\n",
      "--Training for New Delhi--\n",
      "\n",
      "accuracy : 0.7027027027027027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.93      0.83        28\n",
      "           1       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.70        37\n",
      "   macro avg       0.37      0.46      0.41        37\n",
      "weighted avg       0.56      0.70      0.62        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookback = 15\n",
    "results = {}\n",
    "\n",
    "for loc, grp in df.groupby(\"Location\"):\n",
    "    print(f\"--Training for {loc}--\")\n",
    "    g = grp.reset_index(drop=True)\n",
    "    if len(g) < lookback + 10:\n",
    "        continue\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    g_scaled = g.copy()\n",
    "    g_scaled[numeric_cols] = scaler.fit_transform(g[numeric_cols])\n",
    "\n",
    "    x, y = create_seq(g_scaled, lookback, numeric_cols, \"RainTomorrow\")\n",
    "\n",
    "    split = int(0.8 * len(x))\n",
    "    x_train, x_test = x[:split], x[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    classes = np.unique(y_train)\n",
    "    cw_vals = compute_class_weight(\"balanced\", classes = classes, y=y_train)\n",
    "    class_weights = {int(classes[i]) : float(cw_vals[i]) for i in range(len(classes))}\n",
    "\n",
    "    model = Sequential([\n",
    "        Masking(mask_value = 0, input_shape = (lookback, len(numeric_cols))),\n",
    "        LSTM(64, return_sequences = False),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation = 'relu'),\n",
    "        Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer = Adam(0.001), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "    es = EarlyStopping(monitor = \"val_loss\", patience = 3, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs = 20, batch_size = 32, class_weight = class_weights, callbacks = [es], verbose = 1)\n",
    "\n",
    "    y_pred_prob = model.predict(x_test).ravel()\n",
    "    y_pred = (y_pred_prob > 5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\naccuracy : {acc}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3680e0-899e-4d9d-8382-476110176356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94580db7-d1e2-4d6c-bdee-32f1f44ed4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f0e58-0c3b-4be0-93c5-8fed9b15cd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375a1ca-07cf-417b-bd3a-3d4ff73cea9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfff4d8-eccd-4cfc-b480-22506e6ecef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f4fd672-d7be-4b75-8615-26826448c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 12)\n",
      "Data cleaned: (1000, 12)\n",
      "After encoding: (1000, 1013)\n",
      "Train target distribution:\n",
      "RainTomorrow\n",
      "0    580\n",
      "1    220\n",
      "Name: count, dtype: int64\n",
      "After SMOTE:\n",
      "RainTomorrow\n",
      "0    580\n",
      "1    580\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=============== CLASSIFICATION REPORT ===============\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90       145\n",
      "           1       0.71      0.84      0.77        55\n",
      "\n",
      "    accuracy                           0.86       200\n",
      "   macro avg       0.82      0.85      0.83       200\n",
      "weighted avg       0.87      0.86      0.86       200\n",
      "\n",
      "\n",
      "=============== CONFUSION MATRIX ===============\n",
      "\n",
      "[[126  19]\n",
      " [  9  46]]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. IMPORTS\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ============================================\n",
    "# 2. LOAD DATA\n",
    "# ============================================\n",
    "df = pd.read_csv(\"rain.csv\")   # or your rain dataset\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "# ============================================\n",
    "# 3. CLEAN DATA\n",
    "# ============================================\n",
    "\n",
    "# Drop columns with too many missing values\n",
    "df = df.drop(columns=[\"Evaporation\",\"Sunshine\",\"Cloud3pm\",\"Cloud9am\"], errors=\"ignore\")\n",
    "\n",
    "# Drop rows missing the target\n",
    "df = df.dropna(subset=[\"RainTomorrow\"])\n",
    "\n",
    "# Convert target to binary\n",
    "df[\"RainTomorrow\"] = df[\"RainTomorrow\"].map({\"Yes\":1,\"No\":0})\n",
    "\n",
    "# Fill numeric missing values with mean\n",
    "for col in df.select_dtypes(include=np.number):\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# Fill categorical missing with mode\n",
    "for col in df.select_dtypes(include=\"object\"):\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "print(\"Data cleaned:\", df.shape)\n",
    "\n",
    "# ============================================\n",
    "# 4. ONE-HOT ENCODE CATEGORICAL\n",
    "# ============================================\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "print(\"After encoding:\", df.shape)\n",
    "\n",
    "# ============================================\n",
    "# 5. SPLIT DATA\n",
    "# ============================================\n",
    "X = df.drop(\"RainTomorrow\", axis=1)\n",
    "y = df[\"RainTomorrow\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# ============================================\n",
    "# 6. FIX IMBALANCE WITH SMOTE\n",
    "# ============================================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After SMOTE:\")\n",
    "print(y_train_bal.value_counts())\n",
    "\n",
    "# ============================================\n",
    "# 7. SCALE FEATURES\n",
    "# ============================================\n",
    "scaler = StandardScaler()\n",
    "X_train_bal = scaler.fit_transform(X_train_bal)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ============================================\n",
    "# 8. TRAIN XGBOOST MODEL\n",
    "# ============================================\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# ============================================\n",
    "# 9. EVALUATE\n",
    "# ============================================\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n=============== CLASSIFICATION REPORT ===============\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=============== CONFUSION MATRIX ===============\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7fb95-11e4-458c-a6d8-0cb0a5c06a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756eca8-d416-4bba-8819-2e0e77bf8171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4bb25-2de3-4834-9daa-7286f0dc3732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f356cf-45bc-44f6-aa18-c9dc9cbca470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
      "C:\\Users\\kanan\\AppData\\Local\\Temp\\ipykernel_19924\\1187580270.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ts = df.groupby(\"Location\").apply(add_time_features).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape with time-series features: (930, 138)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m X_train_np \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     98\u001b[0m y_train_np \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m---> 99\u001b[0m X_train_bal, y_train_bal \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mfit_resample(X_train_np, y_train_np)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# 9. SCALE FEATURES\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[0;32m    105\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\imblearn\\base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\imblearn\\base.py:99\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m     97\u001b[0m check_classification_targets(y)\n\u001b[0;32m     98\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m---> 99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\imblearn\\base.py:157\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    155\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    156\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 157\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\u001b[38;5;28mself\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1371\u001b[0m     X,\n\u001b[0;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1373\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1374\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1375\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1376\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1377\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1378\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[0;32m   1379\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1380\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1381\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1382\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1383\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1385\u001b[0m )\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'Timestamp'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. IMPORTS\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ============================================\n",
    "# 2. LOAD DATA\n",
    "# ============================================\n",
    "df = pd.read_csv(\"rain.csv\", parse_dates=[\"Date\"])\n",
    "df = df.sort_values([\"Location\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# Convert target\n",
    "df[\"RainTomorrow\"] = df[\"RainTomorrow\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# ============================================\n",
    "# 3. CLEAN DATA\n",
    "# ============================================\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "for col in df.select_dtypes(include=\"object\"):\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# ============================================\n",
    "# 4. FUNCTION TO ADD TIME-SERIES FEATURES\n",
    "# ============================================\n",
    "\n",
    "def add_time_features(g):\n",
    "    g = g.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # Lag features\n",
    "    for lag in [1, 2, 3, 7, 14]:\n",
    "        for col in numeric_cols:\n",
    "            g[f\"{col}_lag{lag}\"] = g[col].shift(lag)\n",
    "\n",
    "    # Rolling means\n",
    "    for win in [3, 7, 14]:\n",
    "        for col in numeric_cols:\n",
    "            g[f\"{col}_roll{win}\"] = g[col].rolling(win).mean()\n",
    "\n",
    "    # Rolling std\n",
    "    for win in [7, 14]:\n",
    "        for col in numeric_cols:\n",
    "            g[f\"{col}_std{win}\"] = g[col].rolling(win).std()\n",
    "\n",
    "    # Rolling min/max\n",
    "    for win in [7, 14]:\n",
    "        for col in numeric_cols:\n",
    "            g[f\"{col}_min{win}\"] = g[col].rolling(win).min()\n",
    "            g[f\"{col}_max{win}\"] = g[col].rolling(win).max()\n",
    "\n",
    "    return g\n",
    "\n",
    "# ============================================\n",
    "# 5. APPLY FEATURES PER LOCATION\n",
    "# ============================================\n",
    "\n",
    "df_ts = df.groupby(\"Location\").apply(add_time_features).reset_index(drop=True)\n",
    "df_ts = df_ts.dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"New shape with time-series features:\", df_ts.shape)\n",
    "\n",
    "# ============================================\n",
    "# 6. ONE-HOT ENCODE CATEGORICAL\n",
    "# ============================================\n",
    "df_ts = pd.get_dummies(df_ts, drop_first=True)\n",
    "\n",
    "# ============================================\n",
    "# 7. TRAIN/TEST SPLIT (TIME-AWARE)\n",
    "# ============================================\n",
    "\n",
    "train_size = int(0.8 * len(df_ts))\n",
    "\n",
    "train = df_ts.iloc[:train_size]\n",
    "test = df_ts.iloc[train_size:]\n",
    "\n",
    "X_train = train.drop(\"RainTomorrow\", axis=1)\n",
    "y_train = train[\"RainTomorrow\"]\n",
    "\n",
    "X_test = test.drop(\"RainTomorrow\", axis=1)\n",
    "y_test = test[\"RainTomorrow\"]\n",
    "\n",
    "# ============================================\n",
    "# 8. BALANCE WITH SMOTE\n",
    "# ============================================\n",
    "sm = SMOTE()\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train_np, y_train_np)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 9. SCALE FEATURES\n",
    "# ============================================\n",
    "scaler = StandardScaler()\n",
    "X_train_bal = scaler.fit_transform(X_train_bal)\n",
    "# Convert test data to numpy array as well\n",
    "X_test_np = X_test.to_numpy()\n",
    "X_test = scaler.transform(X_test_np)\n",
    "\n",
    "# ============================================\n",
    "# 10. TRAIN XGBOOST WITH TIME-SERIES FEATURES\n",
    "# ============================================\n",
    "model = XGBClassifier(\n",
    "    n_estimators=350,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    # Add this parameter to handle the data type issue\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# ============================================\n",
    "# 11. EVALUATE\n",
    "# ============================================\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n========== CLASSIFICATION REPORT =============\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n========== CONFUSION MATRIX =============\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f30ad9-2524-409d-b674-f9823cf6e3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12263a4-8bac-4c68-919a-74703e8c1d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9e9bc-83f9-4509-ac20-22b2b76700d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c6c51-676e-4169-986b-587947d62e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42333c-5ecd-430d-b850-d2e6330a6c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e491df-1d26-4d48-bce9-a84b055c9f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b7203fc-6f5e-4249-958f-b6a2c29dbd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (986, 14, 9)\n",
      "y shape: (986,)\n",
      "Class Weights: {0: np.float64(0.696113074204947), 1: np.float64(1.7747747747747749)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">18,944</span> \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " lstm (\u001b[38;5;33mLSTM\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m18,944\u001b[0m \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m12,416\u001b[0m \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                        \u001b[38;5;34m528\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m17\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,905</span> (124.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,905\u001b[0m (124.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,905</span> (124.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,905\u001b[0m (124.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.6095 - loss: 0.6812 - val_accuracy: 0.4684 - val_loss: 0.6952\n",
      "Epoch 2/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6762 - loss: 0.6725 - val_accuracy: 0.4494 - val_loss: 0.6931\n",
      "Epoch 3/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7016 - loss: 0.6696 - val_accuracy: 0.4747 - val_loss: 0.6944\n",
      "Epoch 4/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7032 - loss: 0.6664 - val_accuracy: 0.4747 - val_loss: 0.7021\n",
      "Epoch 5/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6175 - loss: 0.6622 - val_accuracy: 0.4747 - val_loss: 0.7059\n",
      "Epoch 6/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6587 - loss: 0.6573 - val_accuracy: 0.4873 - val_loss: 0.7138\n",
      "Epoch 7/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6206 - loss: 0.6531 - val_accuracy: 0.4557 - val_loss: 0.7275\n",
      "Epoch 8/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6190 - loss: 0.6504 - val_accuracy: 0.4557 - val_loss: 0.7345\n",
      "Epoch 9/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6381 - loss: 0.6414 - val_accuracy: 0.4873 - val_loss: 0.7311\n",
      "Epoch 10/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6571 - loss: 0.6359 - val_accuracy: 0.4241 - val_loss: 0.7668\n",
      "Epoch 11/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5984 - loss: 0.6392 - val_accuracy: 0.4937 - val_loss: 0.7504\n",
      "Epoch 12/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6651 - loss: 0.6317 - val_accuracy: 0.4367 - val_loss: 0.7656\n",
      "Epoch 13/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6349 - loss: 0.6253 - val_accuracy: 0.4684 - val_loss: 0.7672\n",
      "Epoch 14/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6333 - loss: 0.6181 - val_accuracy: 0.4241 - val_loss: 0.7875\n",
      "Epoch 15/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6540 - loss: 0.6062 - val_accuracy: 0.4937 - val_loss: 0.7704\n",
      "Epoch 16/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6667 - loss: 0.5988 - val_accuracy: 0.4937 - val_loss: 0.7877\n",
      "Epoch 17/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6730 - loss: 0.5821 - val_accuracy: 0.4684 - val_loss: 0.8061\n",
      "Epoch 18/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7127 - loss: 0.5677 - val_accuracy: 0.4620 - val_loss: 0.8363\n",
      "Epoch 19/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6889 - loss: 0.5579 - val_accuracy: 0.4810 - val_loss: 0.8398\n",
      "Epoch 20/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7063 - loss: 0.5457 - val_accuracy: 0.4494 - val_loss: 0.8584\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step \n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.54      0.61       147\n",
      "           1       0.21      0.35      0.26        51\n",
      "\n",
      "    accuracy                           0.49       198\n",
      "   macro avg       0.46      0.45      0.44       198\n",
      "weighted avg       0.58      0.49      0.52       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# --------------------------\n",
    "# 1. Load Data\n",
    "# --------------------------\n",
    "df = pd.read_csv(\"rain.csv\")\n",
    "\n",
    "numeric_cols = [\n",
    "    \"MinTemp\",\"MaxTemp\",\"Humidity9am\",\"Humidity3pm\",\n",
    "    \"Pressure9am\",\"Pressure3pm\",\"WindSpeed9am\",\"WindSpeed3pm\",\"RainToday\"\n",
    "]\n",
    "target_col = \"RainTomorrow\"\n",
    "\n",
    "df = df[numeric_cols + [target_col]].dropna().reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# 2. Encode Target\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "df[\"RainToday\"] = df[\"RainToday\"].map({\"No\": 0, \"Yes\": 1})\n",
    "df[\"RainTomorrow\"] = df[\"RainTomorrow\"].map({\"No\": 0, \"Yes\": 1})\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3. Scale Only Inputs\n",
    "# --------------------------\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# --------------------------\n",
    "# 4. Create Sequences\n",
    "# --------------------------\n",
    "def create_sequences(data, lookback, feature_cols, target_col):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[feature_cols].iloc[i:i+lookback].values)\n",
    "        y.append(data[target_col].iloc[i+lookback])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "lookback = 14  # last 14 days to predict tomorrow\n",
    "X, y = create_sequences(df, lookback, numeric_cols, target_col)\n",
    "\n",
    "print(\"X shape:\", X.shape)   # (samples, 14, 9)\n",
    "print(\"y shape:\", y.shape)   # (samples,)\n",
    "\n",
    "# --------------------------\n",
    "# 5. Train/Test Split\n",
    "# --------------------------\n",
    "split = int(0.8 * len(X))\n",
    "x_train, x_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# --------------------------\n",
    "# 6. Handle Class Imbalance\n",
    "# --------------------------\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# --------------------------\n",
    "# 7. Build LSTM Model\n",
    "# --------------------------\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(lookback, len(numeric_cols))),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --------------------------\n",
    "# 8. Train Model\n",
    "# --------------------------\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 9. Evaluate\n",
    "# --------------------------\n",
    "y_pred_prob = model.predict(x_test).ravel()\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f915a-23f1-44b1-a91c-74c0b5d4c156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246dfdbc-6a1b-47a5-929a-b6b2de4796ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae03e6b-e5cb-49d1-9dbc-396f092fabe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f4c00-dc0e-46b8-9d5f-a957485911c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602212a9-83d0-4bf6-872d-f5f1f8f7a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b767917-efa1-4dba-8eb9-5878e93011ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>20.6</td>\n",
       "      <td>28.7</td>\n",
       "      <td>67</td>\n",
       "      <td>37</td>\n",
       "      <td>1001.4</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>29.3</td>\n",
       "      <td>35.8</td>\n",
       "      <td>46</td>\n",
       "      <td>34</td>\n",
       "      <td>1008.3</td>\n",
       "      <td>1005.7</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>26.0</td>\n",
       "      <td>42.5</td>\n",
       "      <td>74</td>\n",
       "      <td>89</td>\n",
       "      <td>1009.6</td>\n",
       "      <td>998.9</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-04</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.6</td>\n",
       "      <td>94</td>\n",
       "      <td>38</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1002.3</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-05</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>17.3</td>\n",
       "      <td>41.1</td>\n",
       "      <td>67</td>\n",
       "      <td>30</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>999.4</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Location  MinTemp  MaxTemp  Humidity9am  Humidity3pm  \\\n",
       "0  2025-01-01     Mumbai     20.6     28.7           67           37   \n",
       "1  2025-01-02  New Delhi     29.3     35.8           46           34   \n",
       "2  2025-01-03     Mumbai     26.0     42.5           74           89   \n",
       "3  2025-01-04    Kolkata     24.0     39.6           94           38   \n",
       "4  2025-01-05  New Delhi     17.3     41.1           67           30   \n",
       "\n",
       "   Pressure9am  Pressure3pm  WindSpeed9am  WindSpeed3pm RainToday RainTomorrow  \n",
       "0       1001.4       1007.0            10            13        No           No  \n",
       "1       1008.3       1005.7            10             5        No           No  \n",
       "2       1009.6        998.9            16            23       Yes          Yes  \n",
       "3       1010.6       1002.3            16             9        No           No  \n",
       "4       1005.6        999.4            14            23       Yes           No  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"rain.csv\")  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd178b01-a923-411e-bdb2-3737145793fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf5424d-1637-4e9f-a649-f377ee7d96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"Location\", \"RainToday\", \"RainTomorrow\"]\n",
    "\n",
    "le_dict = {}   # store encoders for each column\n",
    "\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    le_dict[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20847faa-2b36-47d8-9d53-175f9f821714",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"MinTemp\", \"MaxTemp\", \"Humidity9am\", \"Humidity3pm\",\n",
    "    \"Pressure9am\", \"Pressure3pm\",\n",
    "    \"WindSpeed9am\", \"WindSpeed3pm\",\n",
    "    \"RainToday\", \"Location\"\n",
    "]\n",
    "\n",
    "target = \"RainTomorrow\"\n",
    "\n",
    "X_raw = df[features]\n",
    "y = df[target].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58309c11-b172-4978-899a-4cf462370823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369128</td>\n",
       "      <td>0.180905</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.953020</td>\n",
       "      <td>0.537688</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.513333</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.731544</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.597315</td>\n",
       "      <td>0.728643</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.286667</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147651</td>\n",
       "      <td>0.804020</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373333</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MinTemp   MaxTemp  Humidity9am  Humidity3pm  Pressure9am  Pressure3pm  \\\n",
       "0  0.369128  0.180905     0.457627     0.118644     0.093333     0.600000   \n",
       "1  0.953020  0.537688     0.101695     0.067797     0.553333     0.513333   \n",
       "2  0.731544  0.874372     0.576271     1.000000     0.640000     0.060000   \n",
       "3  0.597315  0.728643     0.915254     0.135593     0.706667     0.286667   \n",
       "4  0.147651  0.804020     0.457627     0.000000     0.373333     0.093333   \n",
       "\n",
       "   WindSpeed9am  WindSpeed3pm  RainToday  Location  \n",
       "0      0.263158      0.421053        0.0      0.75  \n",
       "1      0.263158      0.000000        0.0      1.00  \n",
       "2      0.578947      0.947368        1.0      0.75  \n",
       "3      0.578947      0.210526        0.0      0.50  \n",
       "4      0.473684      0.947368        1.0      1.00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_scaled, columns=features)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18598e76-7d03-4e4f-9557-bc6298f85853",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# LSTM requires 3D input  (samples, time_steps, features)\n",
    "X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd1e2c7-e1a5-498c-96ed-a6f1d1b74891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: np.float64(0.6896551724137931), 1: np.float64(1.8181818181818181)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y),\n",
    "    y=y\n",
    ")\n",
    "\n",
    "class_weights = {i: weights[i] for i in range(len(np.unique(y)))}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c37696f-cc4b-4ad3-9035-15a670c499f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,200</span> \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " lstm (\u001b[38;5;33mLSTM\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m19,200\u001b[0m \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m2,080\u001b[0m \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m33\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,313</span> (83.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,313\u001b[0m (83.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,313</span> (83.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,313\u001b[0m (83.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(1, len(features))),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b023d1-e150-49fe-9bc4-b136644ff838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6047 - loss: 0.6773 - val_accuracy: 0.7250 - val_loss: 0.6772\n",
      "Epoch 2/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7188 - loss: 0.6575 - val_accuracy: 0.8000 - val_loss: 0.6446\n",
      "Epoch 3/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6906 - loss: 0.6214 - val_accuracy: 0.7125 - val_loss: 0.6118\n",
      "Epoch 4/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7219 - loss: 0.5776 - val_accuracy: 0.7688 - val_loss: 0.5402\n",
      "Epoch 5/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7812 - loss: 0.4969 - val_accuracy: 0.8062 - val_loss: 0.4561\n",
      "Epoch 6/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7828 - loss: 0.4418 - val_accuracy: 0.8562 - val_loss: 0.3974\n",
      "Epoch 7/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8391 - loss: 0.3873 - val_accuracy: 0.8625 - val_loss: 0.3649\n",
      "Epoch 8/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8313 - loss: 0.3543 - val_accuracy: 0.8813 - val_loss: 0.2982\n",
      "Epoch 9/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8391 - loss: 0.3558 - val_accuracy: 0.8813 - val_loss: 0.3299\n",
      "Epoch 10/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8297 - loss: 0.3413 - val_accuracy: 0.8875 - val_loss: 0.2727\n",
      "Epoch 11/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8484 - loss: 0.3432 - val_accuracy: 0.8750 - val_loss: 0.2779\n",
      "Epoch 12/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8516 - loss: 0.3200 - val_accuracy: 0.8750 - val_loss: 0.2919\n",
      "Epoch 13/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8516 - loss: 0.3143 - val_accuracy: 0.8750 - val_loss: 0.3224\n",
      "Epoch 14/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8453 - loss: 0.3373 - val_accuracy: 0.8687 - val_loss: 0.3313\n",
      "Epoch 15/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8469 - loss: 0.3074 - val_accuracy: 0.8875 - val_loss: 0.2638\n",
      "Epoch 16/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8641 - loss: 0.3151 - val_accuracy: 0.8687 - val_loss: 0.3443\n",
      "Epoch 17/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8547 - loss: 0.3039 - val_accuracy: 0.8813 - val_loss: 0.2977\n",
      "Epoch 18/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8516 - loss: 0.3139 - val_accuracy: 0.8938 - val_loss: 0.2598\n",
      "Epoch 19/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8609 - loss: 0.3144 - val_accuracy: 0.8938 - val_loss: 0.2856\n",
      "Epoch 20/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8484 - loss: 0.3003 - val_accuracy: 0.8938 - val_loss: 0.2898\n",
      "Epoch 21/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8594 - loss: 0.2897 - val_accuracy: 0.8938 - val_loss: 0.2827\n",
      "Epoch 22/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8547 - loss: 0.3077 - val_accuracy: 0.8938 - val_loss: 0.2704\n",
      "Epoch 23/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8516 - loss: 0.2986 - val_accuracy: 0.8938 - val_loss: 0.2625\n",
      "Epoch 24/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8562 - loss: 0.2918 - val_accuracy: 0.8938 - val_loss: 0.2731\n",
      "Epoch 25/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8562 - loss: 0.2856 - val_accuracy: 0.8938 - val_loss: 0.2723\n",
      "Epoch 26/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8562 - loss: 0.3003 - val_accuracy: 0.8938 - val_loss: 0.2594\n",
      "Epoch 27/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8500 - loss: 0.3032 - val_accuracy: 0.8938 - val_loss: 0.2783\n",
      "Epoch 28/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8531 - loss: 0.2977 - val_accuracy: 0.8938 - val_loss: 0.2585\n",
      "Epoch 29/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8656 - loss: 0.2859 - val_accuracy: 0.9000 - val_loss: 0.2475\n",
      "Epoch 30/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8469 - loss: 0.2917 - val_accuracy: 0.8875 - val_loss: 0.2904\n",
      "Epoch 31/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8672 - loss: 0.2866 - val_accuracy: 0.9000 - val_loss: 0.2788\n",
      "Epoch 32/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8656 - loss: 0.2854 - val_accuracy: 0.8938 - val_loss: 0.2580\n",
      "Epoch 33/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8656 - loss: 0.2838 - val_accuracy: 0.8938 - val_loss: 0.2787\n",
      "Epoch 34/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8625 - loss: 0.2825 - val_accuracy: 0.8938 - val_loss: 0.2697\n",
      "Epoch 35/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8453 - loss: 0.2958 - val_accuracy: 0.8938 - val_loss: 0.2759\n",
      "Epoch 36/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8703 - loss: 0.2794 - val_accuracy: 0.8938 - val_loss: 0.2526\n",
      "Epoch 37/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8625 - loss: 0.2916 - val_accuracy: 0.8813 - val_loss: 0.2843\n",
      "Epoch 38/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8687 - loss: 0.2847 - val_accuracy: 0.8938 - val_loss: 0.2589\n",
      "Epoch 39/80\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8578 - loss: 0.2728 - val_accuracy: 0.9000 - val_loss: 0.2494\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=80,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d927749-91a5-40fe-9309-4038490259fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8800 - loss: 0.2456 \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13e12dcc-c8d7-42e0-a7b2-1799896b3d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       148\n",
      "           1       0.75      0.81      0.78        52\n",
      "\n",
      "    accuracy                           0.88       200\n",
      "   macro avg       0.84      0.86      0.85       200\n",
      "weighted avg       0.88      0.88      0.88       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b31d29-e656-455b-9d0f-f705c2b2a06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
